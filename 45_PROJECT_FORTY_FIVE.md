# Server Management for Data Scraping

---

### **Directory Creation & Organization:**

1. Create a directory named "scraped_data".
2. Inside "scraped_data", create 12 subdirectories, each named after a month (e.g., "January", "February", etc.).
3. Inside each month’s directory, create files to store data scraped from various websites (e.g., "data_day_1.json", "data_day_1.csv", "data_day_1.xml").

---

### **Move and Rename Files:**

1. Move a data file (e.g., "data_day_1.json") from one month's directory to another (e.g., from "January" to "February").
2. Rename a file within a directory (e.g., rename "data_day_1.json" to "data_day_1_backup.json") and verify the change by listing the contents of the directory.

---

### **Navigation & Listing Files:**

1. Navigate to the "scraped_data" directory.
2. List the contents of the directory and verify the presence of all subdirectories for each month (e.g., "January", "February") and the relevant scraped data files created in the previous task.

---

### **File Permissions Management:**

1. Create a file named "scraping_config.txt" in the "scraped_data" directory to store configuration settings for the automated scraping system.
2. Change the permissions of the file so that only the user has read, write, and execute permissions.
3. Verify the permission changes by listing the file's details.

---

### **Backup Files:**

1. Create a backup of a scraped data file (e.g., "data_day_1.json") by copying it to another directory (e.g., "backup_scraped_data").
2. Verify the copy operation by listing the contents of the backup directory.

---

### **Removing Files & Directories:**

1. Delete an outdated scraped data file (e.g., "data_day_1.json") in the "scraped_data" directory that is no longer needed.
2. Remove an empty subdirectory from one of the months' directories (e.g., remove the "March" directory if it’s empty).

---

### **Creating a Script for File Generation:**

1. Write a script that creates 100 unique data files inside the "scraped_data" directory, each representing a day's worth of data scraped from a website (e.g., "data_day_1.json", "data_day_2.csv").

---

### **Exploring File History:**

1. View the command history to see the last 20 commands executed.
2. Search the history for any command related to file manipulation, data scraping, or file cleanup.

---

### **System Monitoring:**

1. Check the system's uptime to ensure that the data scraping server is continuously running and scraping data.
2. View the system's load and resource usage statistics to ensure the server can handle multiple scraping tasks without performance degradation.

---

### **Checking File Ownership and Permissions:**

1. Check the ownership and group of the "scraping_config.txt" file and verify whether they are correct, ensuring that only authorized personnel can modify the scraping configurations.

---

### **Ping Test & Network Verification:**

1. Verify network connectivity by pinging remote websites from which data is being scraped.
2. Record the response times and verify that the scraping server has reliable access to external websites.

---

### **Search for Specific Files or Content:**

1. Search for a specific data file (e.g., "data_day_15.json") within the "scraped_data" directory.
2. Search for a specific string of text (e.g., "error") inside one of the scraped data files to identify any scraping errors or issues.

---

### **Create a Directory for Each User:**

1. Create a directory for each user or service interacting with the data scraping system (e.g., "user_1_data", "service_2_data") and assign appropriate permissions to store their scraped data.

---

### **Creating a Script for Directory Cleanup:**

1. Write a script that deletes all empty subdirectories from the "scraped_data" directory, ensuring that old, unused directories are removed.

---

### **File Sorting & Management:**

1. Sort scraped data files in the "scraped_data" directory by size and then list the files based on their size, ensuring that larger data files are handled and archived appropriately.
2. Create a report listing the largest and smallest data files in the directory to assist in managing disk space.

---

### **File Type Identification:**

1. Identify and list files of a specific type (e.g., .json, .csv, .xml) in the "scraped_data" directory to organize different types of scraped data for easy access and processing.

---

### **File Compression and Archive:**

1. Compress the "scraped_data" directory into a single archive file to create a backup of all scraped data.
2. Verify the contents of the archive without extracting it, ensuring that all necessary files are included in the backup.

---

### NOTE: Finally fetch all the project commands from history and create a detailed project report "your_name_rollname.md" and push it to git your git repository