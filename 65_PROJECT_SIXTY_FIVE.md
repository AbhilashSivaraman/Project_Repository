# Server Management for Big Data Analytics

---

### **Directory Creation & Organization:**

1. Create a directory named "big_data_analytics".
2. Inside "big_data_analytics", create 12 subdirectories, each named after a month (e.g., "January", "February", etc.).
3. Inside each month’s directory, create files with names corresponding to different types of data analytics reports, such as "customer_segmentation.csv", "sales_performance.parquet", "user_activity_logs.json", or "data_quality_analysis.xlsx".

---

### **Move and Rename Files:**

1. Move a large data analysis file (e.g., "Q1_sales_data.csv") from one month’s directory (e.g., "January") to another (e.g., "February") to organize data based on the time of analysis.
2. Rename a data file (e.g., "raw_data_2023.csv" to "cleaned_data_2023.csv") and verify the change by listing the contents of the directory.

---

### **Navigation & Listing Files:**

1. Navigate to the "big_data_analytics" directory.
2. List the contents of the directory and verify the presence of all subdirectories and large data files created in the previous task.

---

### **File Permissions Management:**

1. Create a file named "sensitive_data.csv" in the "big_data_analytics" directory to store sensitive big data analysis results.
2. Change the permissions of the file so that only authorized users, such as data scientists or analysts, have read, write, and execute access.
3. Verify the permission changes by listing the file's details to ensure that only authorized personnel can modify or access the sensitive data.

---

### **Backup Files:**

1. Create a backup of a critical data file (e.g., "user_behavior_analysis.parquet") by copying it to a backup directory for redundancy.
2. Verify the backup operation by listing the contents of the backup directory to ensure that the data file has been successfully copied.

---

### **Removing Files & Directories:**

1. Delete an outdated data file (e.g., "historical_sales_data_2021.csv") in the "big_data_analytics" directory to keep the data system organized and up to date.
2. Remove an empty subdirectory from one of the months' directories (e.g., "March") if no relevant data analysis has been stored.

---

### **Creating a Script for File Generation:**

1. Write a script that generates 100 big data analysis files (e.g., "customer_segmentation_1.csv", "user_activity_2.json") for different analytics projects or teams.

---

### **Exploring File History:**

1. View the command history to see the last 20 commands executed related to big data file handling, such as creating, editing, or moving reports.
2. Search the history for any command related to file manipulation, analysis, or management of big data files.

---

### **System Monitoring:**

1. Check the system’s uptime to ensure the Big Data Analytics server is actively processing and analyzing large datasets.
2. View the system’s load and resource usage statistics to ensure the server can handle the large-scale data processing required for analytics.

---

### **Checking File Ownership and Permissions:**

1. Check the ownership and group of a large data file (e.g., "customer_behavior_data.csv") and verify whether they are correct to ensure that only authorized users can access sensitive data analytics reports.

---

### **Ping Test & Network Verification:**

1. Verify network connectivity by pinging a remote data source or cloud-based big data platform from which the server fetches analytical datasets.
2. Record the response times and verify the network connectivity to ensure the Big Data server can fetch data quickly for analysis.

---

### **Search for Specific Files or Content:**

1. Search for a specific big data analysis report (e.g., "sales_performance_final.parquet") within the "big_data_analytics" directory to find and review the most recent insights.
2. Search for a specific string of text (e.g., "outliers") inside one of the analysis files to find and explore any data anomalies identified in the dataset.

---

### **Create a Directory for Each User:**

1. Create a directory for each user (e.g., "data_scientist_01", "analyst_03") working with big data in the "big_data_analytics" system to store their individual reports and analysis files.
2. Assign appropriate permissions to ensure that only the respective user and authorized administrators can modify or access their personal big data files.

---

### **Creating a Script for Directory Cleanup:**

1. Write a script that deletes all empty subdirectories from the "big_data_analytics" directory to keep the file system clean and organized after performing data analyses.

---

### **File Sorting & Management:**

1. Sort large data files in a directory by size or modification date and list the files based on the size or most recent updates.
2. Create a report listing the largest and smallest files in the directory, identifying large datasets that might require additional storage resources or archiving.

---

### **File Type Identification:**

1. Identify and list files of specific types (e.g., CSV, JSON, Parquet) in the "big_data_analytics" directory to focus on structured data analysis formats.
2. Filter and list only the files related to a specific analysis category, e.g., "sales_reports.csv" or "user_behavior_analysis.json".

---

### **File Compression and Archive:**

1. Compress the "big_data_analytics" directory into a single archive file for easy backup and storage of all large datasets and analysis reports.
2. Verify the contents of the archive without extracting it to ensure that all relevant reports (e.g., user behavior, sales, and customer segmentation) are included in the backup.

---

### NOTE: Finally fetch all the project commands from history and create a detailed project report "your_name_rollname.md" and push it to git your git repository